{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fcfd4ff-2a13-4cbd-8af8-bf2536f25db0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Bronze layer\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a2a1812-4202-412b-9663-90364213cdc2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Reading all files into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2ae48d-eb02-4c38-b776-ee7890fa921b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_days_df = spark.read.format('json').option('header','true').load(\"dbfs:/FileStore/tables/GHarchive/*.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7a5c28-a14b-4919-92d7-dedaf0534dee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Filtering for dates January 16-31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1318aa4d-181e-4a2e-8794-f098cb71ebf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, col, dayofmonth\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "df = all_days_df.withColumn('created_at_date', col('created_at').cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75757861-1eb7-4555-b869-ea2fd4f31a88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.filter((dayofmonth(col('created_at')) >= 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf18ef5-ba5f-46fc-9b7e-b30d805d87d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Adding Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70cfa127-8d91-4d94-884a-57e19ee027ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, date_format\n",
    "\n",
    "df = df.withColumn('Date', current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb381c95-48ed-446a-8b62-c76f1031d683",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Connecting to ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6009adc8-69f3-4cee-977a-1e04e765ddae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "container_name = \"team3-project2\"\n",
    "storage_account_name = \"20231023desa\"\n",
    "conn_str = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/BronzeLayer\"\n",
    "access_key = \"1PuUNqP+64imQ7kbw5IFioViCAYOQjC18P1tZlFZaN/r1MME7Wgzh+8vbNf1QZg3rCLD6Femlsby+AStdXYJ+g==\"\n",
    "\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\", #set azure account key\n",
    "    access_key # to this access key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c4d5a0e-c389-45ba-b231-0b2ca9359687",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Writing to ADLS in 128mb partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4003715e-95e9-4f5c-81dc-a0808a02e54f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.repartition(18).write.format('parquet').option('header', 'true').partitionBy('created_at_date').mode('overwrite').save(conn_str)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
